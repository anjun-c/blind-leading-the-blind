<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>UNBLIND</title>
</head>
<body>
  <h1>UNBLIND</h1>
  
  <!-- Voice Recording Section (unchanged) -->
  <section>
    <h2>Voice Recording (WebSocket Streaming)</h2>
    <button id="voiceRecordBtn">Start Voice Recording</button>
    <p id="voiceStatus"></p>
    <h3>Transcription:</h3>
    <div id="transcriptionOutput" style="border:1px solid #ccc; padding: 8px; min-height: 40px;"></div>
  </section>
  
  <!-- Video Recording Section (using canvas capture) -->
  <section>
    <h2>Video Recording (WebSocket Streaming)</h2>
    <button id="videoRecordBtn">Start Video Recording</button>
    <p id="videoStatus"></p>
    <h3>Processed Video Frame:</h3>
    <img id="processedFrame" width="320" height="240" alt="Processed Video Frame" style="border:1px solid #ccc;">
  </section>
  
  <script>
    // Global variables for voice streaming (unchanged)
    let voiceRecorder;
    let voiceRecording = false;
    let voiceSocket;
    let voiceStream;

    // Global variables for video streaming using canvas capture
    let videoRecording = false;
    let videoSocket;
    let videoStream;
    let videoCaptureInterval;  // holds our interval timer
    let videoElement;          // hidden video element for playing the stream

    const voiceRecordBtn = document.getElementById('voiceRecordBtn');
    const voiceStatus = document.getElementById('voiceStatus');
    const transcriptionOutput = document.getElementById('transcriptionOutput');
    
    const videoRecordBtn = document.getElementById('videoRecordBtn');
    const videoStatus = document.getElementById('videoStatus');
    const processedFrame = document.getElementById('processedFrame');

    // --- Voice Recording Logic (unchanged) ---
    voiceRecordBtn.addEventListener('click', async () => {
      if (!voiceRecording) {
        voiceStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        voiceSocket = new WebSocket('ws://127.0.0.1:8000/ws/stream_audio');
        voiceSocket.onopen = () => {
          console.log('Voice WebSocket connection opened');
          voiceStatus.innerText = 'Streaming audio...';
        };
        voiceSocket.onerror = (err) => {
          console.error('Voice WebSocket error: ', err);
          voiceStatus.innerText = 'Error in audio stream.';
        };
        voiceSocket.onclose = () => {
          console.log('Voice WebSocket connection closed');
        };
        voiceSocket.onmessage = (event) => {
          const transcription = event.data;
          console.log("Received transcription:", transcription);
          transcriptionOutput.innerText = transcription;
          // Control logic: if transcription contains control phrases
          if (transcription.toUpperCase().includes("ECHO") && !videoRecording) {
            console.log("Detected 'ECHO' - starting video stream");
            startVideoStream();
          }
          if (transcription.toUpperCase().includes("STOP") && videoRecording) {
            console.log("Detected 'STOP' - stopping video stream");
            stopVideoStream();
          }
        };
        
        voiceRecording = true;
        voiceRecordBtn.innerText = 'Stop Voice Recording';
        startVoiceRecordingCycle();
      } else {
        voiceRecording = false;
        if (voiceRecorder && voiceRecorder.state !== "inactive") {
          voiceRecorder.stop();
        }
        if (voiceSocket && voiceSocket.readyState === WebSocket.OPEN) {
          voiceSocket.close();
        }
        voiceRecordBtn.innerText = 'Start Voice Recording';
        voiceStatus.innerText = 'Audio streaming stopped.';
      }
    });

    function startVoiceRecordingCycle() {
      if (!voiceRecording) return;
      
      voiceRecorder = new MediaRecorder(voiceStream, { mimeType: "audio/webm;codecs=opus" });
      
      voiceRecorder.ondataavailable = (event) => {
        if (event.data.size > 0 && voiceSocket.readyState === WebSocket.OPEN) {
          console.log("Sending audio blob of size:", event.data.size);
          voiceSocket.send(event.data);
        }
      };

      voiceRecorder.onstop = () => {
        if (voiceRecording) {
          setTimeout(startVoiceRecordingCycle, 200);
        }
      };

      voiceRecorder.start();
      setTimeout(() => {
        if (voiceRecorder.state === "recording") {
          voiceRecorder.stop();
        }
      }, 3000);
    }

    // --- Video Recording Logic Using Canvas Capture ---
    async function startVideoStream() {
      if (videoRecording) return;
      // Request video (and audio, if needed) stream
      videoStream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
      // Create a hidden video element to play the stream
      videoElement = document.createElement('video');
      videoElement.srcObject = videoStream;
      videoElement.play();
      console.log("Video element created and playing.");
      
      // Open WebSocket connection for video streaming
      videoSocket = new WebSocket('ws://127.0.0.1:8000/ws/stream_video');
      videoSocket.onopen = () => {
        console.log('Video WebSocket connection opened');
        videoStatus.innerText = 'Streaming video...';
      };
      videoSocket.onerror = (err) => {
        console.error('Video WebSocket error: ', err);
        videoStatus.innerText = 'Error in video stream.';
      };
      videoSocket.onclose = () => {
        console.log('Video WebSocket connection closed');
      };

      videoSocket.onmessage = (event) => {
        console.log("Received processed video frame data, size:", event.data.byteLength);
        const blob = new Blob([event.data], { type: 'image/jpeg' });
        console.log("Created blob from processed frame, size:", blob.size);
        const url = URL.createObjectURL(blob);
        processedFrame.src = url;
        // Allow the image to display before revoking the URL
        setTimeout(() => {
          console.log("Revoking URL:", url);
          URL.revokeObjectURL(url);
        }, 1000);
      };

      // Create an offscreen canvas to capture frames
      const canvas = document.createElement('canvas');
      canvas.width = 320;
      canvas.height = 240;
      const ctx = canvas.getContext('2d');
      
      // Start capturing frames at 1 frame per second
      videoCaptureInterval = setInterval(() => {
        if (!videoRecording) {
          clearInterval(videoCaptureInterval);
          return;
        }
        ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
        canvas.toBlob((blob) => {
          if (blob && videoSocket.readyState === WebSocket.OPEN) {
            console.log("Sending captured video frame blob, size:", blob.size);
            videoSocket.send(blob);
          } else {
            console.warn("Blob is null or videoSocket not open.");
          }
        }, 'image/jpeg');
      }, 1000);
      
      videoRecordBtn.innerText = 'Stop Video Recording';
      videoRecording = true;
    }

    function stopVideoStream() {
      if (videoCaptureInterval) {
        clearInterval(videoCaptureInterval);
      }
      if (videoSocket && videoSocket.readyState === WebSocket.OPEN) {
        videoSocket.close();
      }
      videoRecordBtn.innerText = 'Start Video Recording';
      videoStatus.innerText = 'Video streaming stopped.';
      if (videoStream) {
        videoStream.getTracks().forEach(track => track.stop());
      }
      if (videoElement) {
        videoElement.pause();
        videoElement.srcObject = null;
      }
      processedFrame.src = '';
      videoRecording = false;
    }

    // Optional manual control for video recording via button click
    videoRecordBtn.addEventListener('click', async () => {
      if (!videoRecording) {
        await startVideoStream();
      } else {
        stopVideoStream();
      }
    });
  </script>
</body>
</html>
